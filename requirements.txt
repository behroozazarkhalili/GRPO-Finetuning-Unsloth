# GRPO Trainer Requirements
# Object-Oriented GRPO Trainer for Llama 3.2 3B Mathematical Reasoning

# Core ML Libraries
torch>=2.0.0
transformers>=4.30.0
datasets>=2.10.0
accelerate>=0.20.0

# Training and Fine-tuning
trl==0.18.1
peft>=0.4.0
bitsandbytes>=0.41.0

# Unsloth for efficient training
unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git
unsloth_zoo

# Fast inference
vllm>=0.2.0

# Data handling and utilities
numpy>=1.21.0
pandas>=1.3.0
safetensors>=0.3.0
huggingface_hub>=0.15.0

# Configuration and utilities
pyyaml>=6.0
packaging>=21.0
psutil>=5.8.0
tqdm>=4.64.0
filelock>=3.8.0
fsspec>=2022.5.0

# Template and text processing
jinja2>=3.0.0
regex>=2022.1.18

# Networking and web
requests>=2.28.0
urllib3>=1.26.0

# CUDA dependencies (automatically installed with PyTorch CUDA)
# nvidia-cublas-cu12
# nvidia-cuda-cupti-cu12
# nvidia-cuda-nvrtc-cu12
# nvidia-cuda-runtime-cu12
# nvidia-cudnn-cu12
# nvidia-cufft-cu12
# nvidia-cufile-cu12
# nvidia-curand-cu12
# nvidia-cusolver-cu12
# nvidia-cusparse-cu12
# nvidia-cusparselt-cu12
# nvidia-nccl-cu12
# nvidia-nvjitlink-cu12
# nvidia-nvtx-cu12

# Math and scientific computing
sympy>=1.11.0
networkx>=2.8.0

# Additional utilities for advanced features
typing-extensions>=4.0.0
triton>=2.0.0

# Optional: For enhanced performance and features
# xformers>=0.0.20  # Uncomment for memory-efficient attention
# flash-attn>=2.0.0  # Uncomment for Flash Attention support

# Development and testing (optional)
# pytest>=7.0.0
# black>=22.0.0
# flake8>=5.0.0
# mypy>=1.0.0 